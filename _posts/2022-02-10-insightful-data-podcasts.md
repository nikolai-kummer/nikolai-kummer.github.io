---
title:  "Data Podcast Recommendations"
date:   2022-02-10 20:11:00 -0700
categories: 
 -data science 
 -podcasts
excerpt: A list of insightful podcasts in a sea of podcasts!
header:
 og_image: /images/DoughDog.png
 teaser: /images/DoughDog.png
---

I started listening to data podcasts with Partially Derivative back in 2014. The podcast was hosted by Chris Albon, Jonathan Morgan, and later Vidya Spandana, which are now all big names in the data space. Back then you would have to wait weeks to listen to another data podcast. Now there are more episodes of various series than you have time for. This list will be a curated list of insightful episodes that are either technically intresting or insightful. I will keep updating this list as time goes on (starting now).

---
 
 [Towards Data Science Scary Smart: A former Google execâ€™s perspective on AI risk](https://towardsdatascience.com/scary-smart-a-former-google-execs-perspective-on-ai-risk-277bd89549a6)
 
 ![Towards Data Science](/images/data_podcast_towards_data_science.jpg){:height="100px" .align-right} 
The topic of AI safety is frequently covered and few interviews mention anything truly novel. This particular episode is different. Mo Gawdat (a former Chief Business Officer at Google \[X\]) shares his experiences of seeing rapid evolution in the AI landscape.  Many of us tend to think of our work as simple statistical models, but Mo makes a point that they are so much more given their ability to come up with strategies and inferences that they were not programmed for. We have also given the existing systems a lot of power over our behaviours and what news we see, which allows the internet to reshape our world view. Extrapolating into the future (and assuming a doubling rate of model development) we will end up with incredibly powerful systems that will dward human intelligence. At that point it will become very difficult to control the AI agents in any meaningful way.

Mo likened the current AI agents to a toddler and like any toddler, they are currently getting shaped by what we show them. I thought this was a very interesting analogy. He suggested that we can potentially guide this toddler by what the training data that we present them. Currently we show that outrage breeds reward and that violent/angry disagreement is the way to go on the internet. We currently have power over the training data that we generate for the AI agents, so let's make the best of it. What would an internet without outrage event look like? This was a very intresting episode and worth the 60 minutes.


---
 